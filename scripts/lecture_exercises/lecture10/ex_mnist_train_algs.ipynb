{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training algorithms example: MNIST dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading the data from MNIST dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n",
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(train_images.shape, test_images.shape)\n",
    "print(len(train_labels), len(test_labels))\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create the model based on a DNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(256, activation='relu'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "network.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train with SGD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 5s 9ms/step - loss: 0.3975 - accuracy: 0.8925\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3364 - accuracy: 0.9056\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.3031 - accuracy: 0.9145\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2798 - accuracy: 0.9208\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2620 - accuracy: 0.9259\n",
      "0.2450302541255951 0.9311000108718872\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "alg_optimizer = optimizers.SGD(learning_rate=0.01, momentum=0.0)\n",
    "# Optimizers are used with compile and fit method and are used to change the attributes of the ML/DL model such as weights and learning rate in order to reduce the losses. Optimizers help to get results faster.\n",
    "\n",
    "# SGD -> Stochastic gradient descent, performs a parameter update for each training and label example\n",
    "# There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.\n",
    "\n",
    "# lr -> learning rate, is the most important hyperparameter as it controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "# The default learning rate value is 0.001, the recommended value for starting.\n",
    "\n",
    "network.compile(optimizer= alg_optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels, verbose = 0)\n",
    "print(test_loss, test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Zy1rXilMdP2I",
    "outputId": "6b8392fb-a834-4cb1-9776-433017b203c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0635 - accuracy: 0.9821\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0538 - accuracy: 0.9851\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.0457 - accuracy: 0.9878\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0398 - accuracy: 0.9893\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0341 - accuracy: 0.9912\n",
      "0.06510666012763977 0.9807999730110168\n"
     ]
    }
   ],
   "source": [
    "# Train with SGD with momentum\n",
    "\n",
    "algo1 = optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "# Momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "\n",
    "network.compile(optimizer= algo1,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels, verbose = 0)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train with RMSprop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "gFnI8SBeeUbE",
    "outputId": "2b7e2524-2fe0-4453-ef03-4300261a2cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 6s 11ms/step - loss: 2.1257 - accuracy: 0.8697\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.2113 - accuracy: 0.9392\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.1647 - accuracy: 0.9520\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.1382 - accuracy: 0.9602\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.1221 - accuracy: 0.9652\n",
      "0.13997267186641693 0.9634000062942505\n"
     ]
    }
   ],
   "source": [
    "algo2 = optimizers.RMSprop(learning_rate=0.01, rho=0.99)\n",
    "\n",
    "network.compile(optimizer= algo2,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels, verbose = 0)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train with Adam"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 10s 18ms/step - loss: 0.0613 - accuracy: 0.9814\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.0463 - accuracy: 0.9856\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0384 - accuracy: 0.9880\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.0332 - accuracy: 0.9894\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 0.0282 - accuracy: 0.9911\n",
      "0.11560291796922684 0.9722999930381775\n"
     ]
    }
   ],
   "source": [
    "algo3 = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "network.compile(optimizer= algo3,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels, verbose = 0)\n",
    "print(test_loss, test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Batch normalization\n",
    "Is a technique for training very deep neural networks that normalizes the contributions to a layer for every mini-batch. This has the impact of settling the learning process and drastically decreasing the number of training epochs required to train deep neural networks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "3oRYTjH9fmVv",
    "outputId": "294a08e4-0b95-48f2-9cbf-9f12aec9ee6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 538,890\n",
      "Trainable params: 537,354\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1027, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 527, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1140, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 634, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1166, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1213, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 216, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable dense_6/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.{self.__class__.__name__}.'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [14], line 18\u001B[0m\n\u001B[0;32m     12\u001B[0m network_bn\u001B[38;5;241m.\u001B[39msummary()\n\u001B[0;32m     14\u001B[0m network_bn\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m algo3,\n\u001B[0;32m     15\u001B[0m                 loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     16\u001B[0m                 metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 18\u001B[0m network_bn\u001B[38;5;241m.\u001B[39mfit(train_images, train_labels, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m)\n\u001B[0;32m     20\u001B[0m test_loss, test_acc \u001B[38;5;241m=\u001B[39m network_bn\u001B[38;5;241m.\u001B[39mevaluate(test_images, test_labels, verbose \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(test_loss, test_acc)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3dem6ll_.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mKeyError\u001B[0m: in user code:\n\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1027, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 527, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1140, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 634, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1166, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1213, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"C:\\Users\\anaca\\Documents\\GitHub\\SIB-ML-Portfolio\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 216, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable dense_6/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.{self.__class__.__name__}.'\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "\n",
    "network_bn = models.Sequential()\n",
    "\n",
    "network_bn.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network_bn.add(BatchNormalization())\n",
    "network_bn.add(layers.Dense(256, activation='relu'))\n",
    "network_bn.add(BatchNormalization())\n",
    "network_bn.add(layers.Dense(10, activation='softmax'))\n",
    "# Apply a batch normalization for each layer.\n",
    "\n",
    "network_bn.summary()\n",
    "\n",
    "network_bn.compile(optimizer= algo3,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network_bn.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network_bn.evaluate(test_images, test_labels, verbose = 0)\n",
    "print(test_loss, test_acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ex_mnist_train_algs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
